{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLHW.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYEMjbMPdWOx4amN6GL5FC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gerviba/deeplearning-human-posture-recogn/blob/master/DLHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWPZsbgRpM2p"
      },
      "source": [
        "# Deeplearning Project\n",
        "\n",
        "Team name: **Uristen Very Deep**\n",
        "\n",
        "Members:\n",
        "- **Gloria** Benő\n",
        "- **Ádám** Koncz\n",
        "- **Gergely** Szabo\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoAwuC6fsWLO"
      },
      "source": [
        "## 1) Human Posture Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83AfKmJOqRqI"
      },
      "source": [
        "### 1.1) The idea\n",
        "\n",
        "It would be fun to create an application that can recognize several postures of a human body. For example Xbox Kinect do a similar thing (but they use infrared and depth cameras). Our goal is to something like that, but without fancy hardware. Some interesting usecases could be a **Mortal Combat controller using a webcam** or the **Chrome T-Rex game using smartphone camera**.\n",
        "\n",
        "If we can reliably recognise certain postures we can aggregate them to recognise movements and make the result more interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRjk56XGsSv1"
      },
      "source": [
        "### 1.2) Data collection thouts\n",
        "\n",
        "- There are different approaches to solve this problem. We looked up for similar projects. Some projects uses videos to identify movements some uses series of pictures, some uses data from IR and depth camera.\n",
        "\n",
        "- We selected the poisture recognition, because we think it is the less complex way of doing it (and it requires less resource and can be used from a smartphone).\n",
        "\n",
        "- We wanted to create training data with clean background. Because we can add more noise in the future if needed, and we also want to filter the 'noisy' backgrounds out in the end.\n",
        "\n",
        "- We can also apply grayscale and shear transformation to make the input data more rich.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8CFAParsZUO"
      },
      "source": [
        "### 1.3) The collected data - basic transormation\n",
        "\n",
        "- So we shoot symmetric 26 photos for each 15 agents. Then we combined the photos into 676 different postures per agent.\n",
        "\n",
        "- We made a python script (it can be found in this git repo as well) that generates **all the possible postures from the symmetric input set**. We named all the postures. (See topic **1.4** and **1.5**)\n",
        "\n",
        "#### An example of the generated picture (Agent: 03, Left-Figure: 1-1, Right-Figure: 1-2)\n",
        "![generated A03_LF11_RF12](http://test1.ortos.ch/deeplearning/train/LF1-1_RF1-2/LF1-1_RF1-2_A03.jpg)\n",
        "\n",
        "#### Another example of the generated picture (Agent: 10, Left-Figure: 1-1, Right-Figure: 2-4)\n",
        "![generated A10_LF11_RF24](http://test1.ortos.ch/deeplearning/train/LF1-1_RF2-4/LF1-1_RF2-4_A10.jpg)\n",
        "\n",
        "- We also shoot 18 asymmetric photos with 13 agents.\n",
        "\n",
        "- The train and the validation dataset consists of 10374 images.\n",
        "\n",
        "- We also took some test photos with higher background noise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPzHOr8gyjYX"
      },
      "source": [
        "#### 1.4) Figures 1-6 (Symmetric ones)\n",
        "\n",
        "- The green is the **left** and teh red is the **right** hand. With all the subfigures there are 26 postures as mentioned earlier. (And there are 676 combinations)\n",
        "\n",
        "![fig1-6](http://test1.ortos.ch/deeplearning/figures_symm.png)\n",
        "\n",
        "#### 1.5) Figures 7-12 (Asymmetric ones)\n",
        "\n",
        "- The green is the **left** and teh red is the **right** hand in here as well. There are 18 postures.\n",
        "\n",
        "![fig7-12](http://test1.ortos.ch/deeplearning/figures_asymm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmmf-rdWsZO2"
      },
      "source": [
        "\n",
        "### 1.6) Remote storage\n",
        "\n",
        "The data is consists of more than 10000 annotated images so we decided to upload it to a cloud data storage.\n",
        "\n",
        "> The root directory is http://test1.ortos.ch/deeplearning/\n",
        "\n",
        "- The raw data can be found in the [/raw](http://test1.ortos.ch/deeplearning/raw/) directory. \n",
        "- The annotated but not converted data can be found in the [/annotated](http://test1.ortos.ch/deeplearning/annotated/) directory. \n",
        "- The transformed data can be found in the [/train](http://test1.ortos.ch/deeplearning/train/), [/validate](http://test1.ortos.ch/deeplearning/validate/) and the [/test](http://test1.ortos.ch/deeplearning/test/) directory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC0Z-wPHsZLm"
      },
      "source": [
        "### 1.7) Further thouts\n",
        "\n",
        "In theory that many data will be enough and the generated data is good enough to make our project work.\n",
        "\n",
        "This might be false and in this case we might need to further clean our input data or create more."
      ]
    }
  ]
}